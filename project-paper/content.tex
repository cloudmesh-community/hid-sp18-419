\title{Benchmarking a Sentiment Analysis Algorithm Using Hadoop on Multiple 
Platforms}


\author{Min Chen}
\affiliation{%
  \institution{Indiana University}
  \streetaddress{School of Informatics, Computing, and Engineering}
  \city{Bloomington}
  \state{IN}
  \postcode{47408}
}
\email{mc43@iu.edu}

\author{Gregor von Laszewski}
\affiliation{%
  \institution{Indiana University}
  \streetaddress{Smith Research Center}
  \city{Bloomington}
  \state{IN}
  \postcode{47408}
}
\email{laszewski@gmail.com}

\author{Bertolt Sobolik}
\affiliation{%
  \institution{Indiana University}
  \streetaddress{School of Informatics, Computing, and Engineering}
  \city{Bloomington}
  \state{IN}
  \postcode{47408}
}
\email{bsobolik@iu.edu}


% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{M. Chen, G. v. Laszewski, B. Sobolik}


\begin{abstract}
A sentiment analysis algorithm is run on several Hadoop
configurations: in pseudo-distributed mode on an Ubuntu 16.04 virtual
machine on a MacBook Pro with and without YARN, in pseudo-distributed
mode in a Docker container on various personal computers and on
FutureSystems Echo, in a cluster created using Docker Compose on
various personal computers and on FutureSystems Echo, and in a cluster
created using Docker Swarm on FutureSystems Echo. Performance tests
are natural language processing based sentiment analysis on movie
reviews (Polarity 2.0) implemented under MapReduce framework using
Hadoop Streaming. Configurations are described in detail and steps to
recreate them are outlined. In an appendix, steps toward creating a
Hadoop cluster on five networked Raspberry Pi 3 model B computers in a
repeatable and scalable fashion, automating as much of the setup
process as possible are detailed, and next steps are discussed.
\end{abstract}

\keywords{hid-sp18-419, hid-sp18-405, Hadoop, Docker, FutureSystems}


\maketitle

\section{Introduction}\label{s:intro}
Various configurations of personal computers are benchmarked on their
performance of a sentiment analysis algorithim written in Python
leveraging the Hadoop framework. Software and hardware technologies
used in this study are described: Hadoop, Docker Compose, Docker
Swarm, VirtualBox, four personal computer configuratons and one cloud
platform. The data set being used for the benchmarking is
described. The sentiment analysis algorithm used for this study used
for is explained in both its original form and in the modified version
that was developed to run in the MapReduce framework. The three
deployments of Hadoop used for benchmarking are destribed:
Pseudo-Distributed mode, Fully-Distributed mode using Docker Compose,
and Fully-Distributed mode using Docker Swarm. The benchmarking
process is explained. Results are presented and discussed, and
conclusions are drawn.

\section{Technology Used}\label{s:techused}

This section describes the technologies that have been utilized
throughout the project. These technologies can be grouped into the
following categories: Hadoop, Docker, personal computers, and cloud
platforms.

\subsection{Hadoop}
Hadoop is a software library supported and maintained by The Apache
Software Foundation. It allows for processing large data sets in a
distributed computing environment. Inspired by two
papers: \textit{Google File
System}~\cite{hid-sp18-405-ghem2003goolefilesystem} published in 2003
and \textit{MapReduce: Simplified Data Processing on Large
Clusters}~\cite{hid-sp18-405-dean2008mapreduce} which followed in
2004, Hadoop 0.1 was released in 2006 and the official 1.0 release
came in 2012~\cite{hid-sp18-405-hadoop-wiki}. Hadoop is designed to
allow users to leverage simple programming models to process large
data sets on computer clusters. The software detects and handles
failures on commodity hardware instead of requiring specialized
hardware to handle deliver high-availibility. The most current version
at the time of writing is
3.0.1~\cite{hid-sp18-405-hadoop-official}. This project will leverage
version 2.9.0, which is the current stable version.

The core of Hadoop include the following components: Hadoop Common,
Hadoop Distributed File System (HDFS), Hadoop YARN and Hadoop
MapReduce. There are also a large variety of Hadoop-related projects
at Apache that cover machine learning (Mahout), data warehousing
(Hive), data serialization (Avro), databases (Hbase, Cassandra),
etc. All of these components are available via the Apache open source
license~\cite{hid-sp18-405-hadoop-official}. A high-level diagram of
the basic Hadoop architecture in a distributed cluster with one Master
Node and two Worker Nodes is shown in Figure~\ref{f:hadoop-high}.

\begin{figure*}[!ht]
	\centering\includegraphics[width=\columnwidth]{images/hadoop.png}
	 \caption{Hadoop High Level 
	 Architecture~\cite{hid-sp18-405-hadoop-wiki}}\label{f:hadoop-high}
\end{figure*}

\paragraph{HDFS} HDFS stores metadata and 
application data separately: metadata are stored on the NameNode
(usually sitting on the Master Node) and application data are
distributively stored on Datanodes. Datanodes communicate with each
other through TCP-based protocols. Instead of storing one copy, data
is replicated on multiple DataNodes to ensure
high-availability~\cite{hid-sp18-405-shvachko2010hdfs}.

\paragraph{YARN} Hadoop YARN provides resource management for the 
Hadoop cluaster~\cite{hid-sp18-405-hadoop-official}. As shown in
Figure~\ref{f:hadoop-high}, based on application demand, priorities,
and resource availability, the Resource Manager dynamically allocates
containers to applications to run on particular
nodes~\cite{hid-sp18-405-vavi2013yarn}. (Containers here meaning
logical bundle of resources, not to be confused with Docker
containers.) YARN interacts with Node Manager on each of the nodes:
monitoring resource availability, reporting faults, and managing
container lifecycles~\cite{hid-sp18-405-vavi2013yarn}.

\paragraph{MapReduce} Hadoop MapReduce is an implementation of the 
MapReduce programming model that is typically used for processing
large amounnts of data~\cite{hid-sp18-405-hadoop-official}. It relies
on YARN for resource management. A map function is specified that
generated an set of intermediate key-value pairs. The output of the
map function is fed to a reduce function that combines these
intermediate values into the final
result~\cite{hid-sp18-405-dean2008mapreduce}.

\paragraph{Hadoop-Streaming} Although Hadoop and most of the
applications in the Hadoop Framework are written in Java,
Hadoop-Streaming, a java program that comes with the Hadoop
distribution. Using Hadoop-Streaming, one can define mapper and
reducer functions written in another language, and feed these
into \verb|hadoop-streaming.jar| as parameters to create and run
MapReduce jobs~\cite{hid-sp18-405-hadoop-streaming}. In this project,
all the mappers and reducers are written in Python and passed to
Hadoop-Streaming. One drawback to this approach is that if the script
being run is dependent on outside packages, they need to be zipped in
a flat way and passed to each job containers for a special import
before use.  Because of this limitation, some of the linguistic
features of the original version of the sentiment analysis algorithm
used in this project are omitted due to the difficulties of
transporting and loading necessary packages and corpora. This will be
discussed in more detail in Section~\ref{s:algorithm}.


\subsection{Docker}
Docker is a technology that performs operating system-level
virtualization, allowing applications to run in containers instead of
full VMs~\cite{hid-sp18-405-docker-wiki}. It leverages control groups
and name space isolation in the Linux kernel. Containers start up much
faster than VMs and are supported by all the major public cloud
vendors~\cite{Foster:2017:CCS:3158276}. The most current stable
release of Docker Community Edition available at the time of writing,
18.03.0-ce, is used for this project.

\paragraph{Docker Compose} Docker Compose is a tool which is distributed
with Docker. It helps define and manage applications or clusters with
multiple Docker
containers~\cite{hid-sp18-405-docker-compose-doc}. Docker Compose
provides multiple isolated environments on a single host. It is used
in this project so that a Hadoop cluster can be deployed on a single
VM or a single physical machine with the Master Node and Worker Nodes
running in separate containers. The scalability feature of Docker
Compose allows the size of the cluster to be adjusted at run time
without stopping the cluster. When a service is stopped and restarted
Compose will resuse the existing containers as long as the
configuration files have not changed for those
containers~\cite{hid-sp18-405-docker-compose-doc}.

\paragraph{Docker Swarm} A group of physical machines or VMs running 
Docker Engine can be joined into a cluster called a Docker Swarm. Each
machine is referred to as a node. One node in the cluster will be the
head, or manager, of the swarm. The head node is the only node
communicating with outside client and receiving execution
instructions. It coordinates instructions and passes them to other
worker nodes. On FutureSystem Echo, the cloud platform used for this
project, five physical servers are in Docker Swarm mode with one head
and four workers. Key features of Docker Swarm include: cluster
management integrated with Docker Engine, decentralized design,
scaling, load balancing, and rolling
updates~\cite{hid-sp18-405-docker-swarm-doc}. A stack of services can
be deployed on a Swarm cluster using the same YAML file used by Docker
Compose. The user can pass a single command to the head node to start,
scale or stop the services. The head node then coordinates among the
the other nodes in the Swarm cluster and attempt to balance the
workload optimally.

\subsection{VirtualBox}
VirtualBox is software from Oracle that allows one to run x86 and
AMD64/Intel64 VMs on personal computers. It is used in this project
for running the VMs in the Hadoop setups and for burning custom images
for the Raspberry Pis discussed in Section~\ref{s:appendix}. Add-ons
include Guest Additions, which allow one to cut and paste between the
VM and the host, and the VirtualBox Extension Pack that allows
mounting peripherals on USB 3.0~\cite{hid-sp18-419-virtualbox}.

\subsection{Personal Computers}\label{ss:pcs}
\paragraph{Macbook Pro (15-inch, 2017)}
\begin{itemize}
        \item Operating system: macOS Sierra 10.12.6 
        \item Processor: 3.1 GHz Intel Core i7
        \item Memory: 16 GB 2133 MHz LPDDR3
        \item Graphics: Intel HD Graphics 630 1536 MB
        \item Periphals
        \begin{itemize}
                \item HyperDrive - USB Type-C Hub (has microSD card slot)
                \item Insignia - USB Type-C to Gigabit Ethernet
                Adapter (to connect to switch)
        \end{itemize}
        \item Software
        \begin{itemize}
                \item VirtualBox 5.2.8 with Guest Additions and the
                VirtualBox Extension Pack for USB 3.0 support.
        \end{itemize}
\end{itemize}

\paragraph{Macbook Pro (13-inch, 2015)}
\begin{itemize}
	\item Operating system: macOS High Sierra 10.13.4 
	\item Processor: 2.7 GHz Intel Core i5
	\item Memory: 8 GB 1867 MHz DDR3
	\item Graphics: Intel HD Graphics 6100 1536 MB
\end{itemize}

\paragraph{Acer Laptop (Aspire 4830)}
\begin{itemize}
	\item Operating system: CentOS Linux 7 (Core)
	\item Processor: Intel Core i5-2430M CPU @2.40GHz
	\item Memory: 16 GB 1600 MHz DDR3
\end{itemize}

\paragraph{Asus Desktop}
\begin{itemize}
	\item Operating system: Windows 7 Professional
	\item Processor: Intel Core i5-4690K CPU @3.50GHz
	\item Memory: 16 GB 1600 MHz DDR3
	\item Software: VirtualBox 5.2.8
\end{itemize}

\subsection{Cloud Platforms}
\paragraph{FutureSystem Echo}  FutureSystems is available as part of the 
Digital Science Center (DSC) infrastructure and resources located at 
Indiana University - Bloomington~\cite{las18handbook}. One of the 
resources, Echo, is a ``SuperMicro distributed shared memory cluster with 
192 CPU cores and 6TB total memory capacity''~\cite{las18handbook}. 

\begin{itemize}
	\item Operating system: Ubuntu 16.04
	\item Processors: 16 SuperMicro X9DRW servers, each node
        with 2 6-core Intel Xeon CPU E5-2640 2.50GHz
        processors for a total of 192 CPU cores
	\item Memory: 6TB (384GB per node)
\end{itemize}


\section{Data}\label{s:data}

The data used for the project is the Polarity Data 2.0, which is a
dataset of movie reviews first used by Bo Pang and Lillian
Lee~\cite{hid-sp18-405-sentiment-pang2004asentimental}
~\cite{hid-sp18-405-sentiment-pang2002thumbs}.
The dataset includes 1000 positive and 1000 negative processed
reviews that are labeled with respect to their overall sentiment
polarity.

Each of the reviews is in the format of text file; each line in the
files corresponds to a sentence; and every token (i\.e\. words,
punctuation marks, and numbers) has been separated by a
space. Figure~\ref{f:data} shows several lines in one of the movie
reviews with line number on the left, illustrating the two features
mentioned above. These features play an important role in the
sentiment analysis algorithm, especially when using the MapReduce
framework, which is discussed in detail in Section~\ref{s:algorithm}.
\begin{figure*}[!ht]
	\centering\includegraphics[width=\columnwidth]{images/polarity-data.png}
	\caption{Example Data File}\label{f:data}
\end{figure*}

The data is directly pulled from the
source~\cite{hid-sp18-405-sentiment-data} and then split into training
and testing sets according to a ratio of 8:2. The split algorithm
maintains the original ratio of positive and negative reviews in both
the training and testing data sets. As a result, there are 1600
reviews (800 positive and 800 negative) in the training data set and
400 reviews (200 positive and 200 negative) in the testing data
set. The splitting process is done by using the random sort
functionality in bash with a fixed random seed to ensure consistency
of performance benchmarking on different platforms.


\section{Algorithm}\label{s:algorithm}

This section introduces the algorithm used to classify the overall
sentiment of the movie reviews in the benchmarking process. The
algorithm is a modified version of the one used by Pang and
Lee~\cite{hid-sp18-405-sentiment-pang2004asentimental} that is
explained in details by Jurafsky and
Martin~\cite{hid-sp18-405-sentiment-jurafsky2009}.

\subsection{Baseline algorithm}\label{ss:base}

The baseline algorithm contains the following steps:

\begin{enumerate}
	\item Tokenization
	\item Feature Extraction
	\item Classification
\end{enumerate}

\paragraph{Tokenization}
As mentioned in Section~\ref{s:data}, every token (words, punctuation
marks, and numbers) has been separated by space in the text
files. Becuase of this, the algorithm needs only to split the text
strings with space to perform tokenization rather than using
sophisticated tokenizers. This saves space and running time and avoids
the complication of loading Python packages for Hadoop worker nodes in
Hadoop-streaming. However, in more general settings with text data,
the non-trivial tokenization step needs to be performed before other
type of text processing.

\paragraph{Feature Extraction}
The main feature that we use is the words contained in the documents.
However, there are options mentioned in Jurafsky and
Martin~\cite{hid-sp18-405-sentiment-jurafsky2009}, including:
\begin{enumerate}
	\item All tokens
	\item Removing stoplist words
	\item Removing punctuation tokens
	\item Using only adjectives
	\item Unify tokens with lemmas
	\item Negation treatment
\end{enumerate}
In this project, we applied stoplist words removal, punctuation tokens 
removal and negation treatment. 

Stoplist is the list of words that are considered to be commonly used
across documents regardless of sentiments and therefore add little
value when performing classification tasks. We removed stoplist words
according to the English stopwords module from the Python package:
NLTK corpus~\cite{hid-sp18-405-sentiment-stopworddoc}. Punctuation
removal is done using the Python string
module~\cite{hid-sp18-405-sentiment-punctuationdoc}, supplemented by a
regular expression written for this project. We applied a basic
negation treatment by adding a NOT\_ prefix to every token that
follows a negation word and is not separated by any punctuation
tokens. For example:

I really do not like the movie

becomes:

I really do not Not\_like Not\_the Not\_movie.

This method is used by Das and
Chen~\cite{hid-sp18-405-sentiment-das2001yahoo} as well as Pang, Lee,
and Vaithyanathan~\cite{hid-sp18-405-sentiment-pang2002thumbs}.

We did not apply the option of using only adjectives based on the findings
by Pang et\. al\. that using adjectives alone does not provide a better
results than including all
words~\cite{hid-sp18-405-sentiment-pang2004asentimental}. In our own
experiments, the accuracy dropped by an average of 2\% in the testing
phase when we implemented the algorithm. We also did not apply
lemmatization for all the tokens. Although lemmatization would
increase the accuracy by around 2\% in one of the authors previous
studies, it requires ontologies such as Wordnet. Including the Wordnet
corpus with a lemmatizer would have complicated the implementation of
Hadoop-streaming significantly. We tried to follow the instructions
online~\cite{hid-sp18-405-hadoopstreaming-nltk}
~\cite{hid-sp18-405-hadoopstreaming-corpus}
to pass Python package NLTK and the corpus Wordnet as zipped files
from name nodes to data nodes in the MapReduce tasks and found that
the package could be passed but not loaded and corpus could not be
passed at all. The issues we faced deserve further study in a future
project.

Pang, Lee, and
Vaithyanathan~\cite{hid-sp18-405-sentiment-pang2002thumbs}, also
considered different set of features such as bigrams, combination
(back up) of bigrams and unigrams, top-unigrams, etc. For simplicity,
we chose to only use unigram models, i\.e\. single tokens.

\paragraph{Classifier}
We used Naive Bayes as the classifier. The formulation is given as:
\begin{equation}\label{eq:nb}
c_{NB}=\text{argmax}_{c_j \in C} P(c_j) \prod_{i \in positions} P(w_i|c_j)
\end{equation}

where in this case, $j \in \{positive, negative\}$. Also, for this
movie review dataset, we know the prior probabilities:
$P(c_{positive})=P(c_{negative})=0.5$

\paragraph{Smoothing}
To avoid the problem that the probability of some token in the 
training/testing document is zero, we use the add-one smoothing, or Laplace 
smoothing. In this case:
\begin{equation}\label{eq:sm}
P(w|c) = \frac{count(w,c) + 1}{count(c) + |V|}
\end{equation}

Figure~\ref{f:algo} is the algorithm we implemented as summarized by
Jurafsky and Martin:
\begin{figure}[!ht]
		\centering\includegraphics[width=\columnwidth]{images/algorithm.png}
		\caption{Naive Bayes for Sentiment 
		Analysis~\cite{hid-sp18-405-sentiment-jurafsky2009}}\label{f:algo}
\end{figure}

\subsection{Implementation Using MapReduce}

As illustrated in Equation~\ref{eq:nb}, Equation~\ref{eq:sm}, and
Figure\ref{f:algo}, the Naive Bayes classification algorithm is based
on comparing posterior probability. The likelihood of each single
valid token can be calculated by aggregating the count of this token
among the training documents for the two classes: positive and
negative. The likelihood of a testing document is an aggregation
of the likelihood of each valid token within that document. Therefore
both training and testing process can be implemented in a MapReduce
framework to facilitate the process of counting tokens and
aggregation. The implementation is summarized in Figure~\ref{f:mapreduce}

\begin{figure}[!ht]
	\centering\includegraphics[width=\columnwidth]{images/mapreduce.png}
	\caption{MapReduce Implemenation of the Algorithm}\label{f:mapreduce}
\end{figure}

There are two MapReduce jobs in the training process, one for training
on positive-labeled documents and the other for negative-labeled
documents.  $count(w,c)$ in Equation~\ref{eq:sm} shows the necessity
to count the tokens according to the classes, which are positive and
negative in this task.  During the training on positive-labeled
documents, the TrainingMapper Python code will read in each positive
movie review in the training set through standard input, filter valid
tokens (removing stoplist words, punctuation, and adding negation and
described in Subsection\ref{ss:base}) and print to the standard
output. The read-in process is done on a line-by-line basis using a
Python generator. This is achievable because the data is formatted in
so that each line is one sentence (see Section~\ref{s:data}), thus no
sentence is split by line breaks. The TrainingReducer Python code will
take these standard output and perform aggregation by tokens. The
result is a dictionary-like text file containing number of occurrence
for each valid token in the positive training set. The negative
training documents are processed in a similar way.

In the testing process, each testing document is analyzed
separately. The TestingMapper reads in each document, perform the same
filter to keep valid tokens, and print to standard output. The
TestingReducer reads in both this standard output and the two results
files from the training process, calculating the likelihood of each
valid token in this document according to the smoothing formula given
by Equation~\ref{eq:sm} before getting posterior probability through
aggregation according to Equation~\ref{eq:nb}. Then for each document
there are two posterior probabilities, positive and negative. The
document is assigned to the one with higher probability. The final
output is a text file in which each line contains the testing document
name and its classification label. In the actual implementation, we
performed the testing process using two MapReduce jobs, one for the
testing data that are actually positive, the other for the testing
data that are actually negative. This is essentially the same as
performing testing process on each document blindly and then verify
the results. However, implementing it in this way makes it easier to
calculate the accuracy of the algorithm.
 
In summary, we used Python codes to implement two mappers
(TrainingMapper and TestingMapper) and two reducers (TrainingReducer
and TestingReducer) to perform four MapReduce jobs for the training
and testing processes. For the fixed random seed mentioned in
Section\ref{s:data}, we achieved the following results: out of the 200
positive testing data, 162 are correctly labeled; out of the 200
negative testing data, 168 are correctly labeled. Therefore, the
overall accuracy of the algorithm is 82.5\%. This is in line to the
result achieved by Pang, Lee, and Vaithyanatha, which is illustrated
in Figure\ref{f:pang-result}.
% 
%\begin{figure}[!ht]
%	\centering\includegraphics[width=\columnwidth]{images/algoresult.png}
%	\caption{Results of the Algorithm}
%	\label{f:algoresult}
%\end{figure}

\begin{figure}[!ht]
	\centering\includegraphics[width=\columnwidth]{images/pang-result.png}
        \caption{Results
	from Pang, Lee, and Vaithyanatha
	(2002)~\cite{hid-sp18-405-sentiment-pang2002thumbs}}\label{f:pang-result}
\end{figure}


\section{Hadoop Deployment}\label{s:hadoopdep}

For the entire project, we chose the current stable release of Hadoop
at the time of the study: version 2.9.0. In order to make it
convenient to benchmark across platforms we adopted a fixed set of
configuration files for all of our installations. For example, the
value of \verb|dfs.replication| is set to 3, which means that 3 copies of
the data will be stored in the HDFS. In addition, physical memory
limit for each map task and reduce task is set to 1024MB, with the
ratio of virtual memory to physical memory allowed set to
2.1. Therefore, the virtual memory limit for each map task and reduce
task is 2150MB.

Hadoop can be configured to run in three modes: Standalone Mode, 
Pseudo-Distributed Mode and Fully-Distributed 
Mode~\cite{hid-sp18-405-hadoop-singlenode}. 
\begin{itemize}
	\item Standalone Mode: In this Mode, Local file system instead of
        HDFS is used. Resource Manager, Node Manager and all other
        processes are running not only in one virtual machine but in one
        single Java Virtual Machine (JVM).
	\item Pseudo-Distributed Mode: HDFS is used, NameNode, DataNode, 
	ResourceManager and NodeManager are in the same virtual machine
        but in different JVMs. 
	\item Fully-Distributed Mode: This is the mode that Hadoop
        clusters can be ranging from a few nodes to large clusters with
        thousands of nodes. Each node (worker) will be on different
        virtual/physical machines and they all have DataNode and
        NodeManager running as seperate JVMs.
\end{itemize}
In this project, we deployed Hadoop in Pseudo-Distributed Mode and 
Fully-Distributed Mode with the main focus on the latter. 

\subsection{Pseudo-Distributed}\label{ss:pseudo-distributed}

We developed Pseudo-Distributed Hadoop cluster in two ways as
illustrated in Figure~\ref{f:hadoop-pseudo}. The direct installation
is on the left, where NameNode, DataNode, ResourceManager and
NodeManager are sitting on the operating system of the host machine
(wich could be a VM) as separate Java Virtual Machines. The right
panel of Figure~\ref{f:hadoop-pseudo} shows the dockerized version
where the four core components are wrapped in one Docker container and
sitting on the Docker Engine.

\begin{figure}[!ht]
	\centering\includegraphics[width=\columnwidth]{images/hadoop-docker-pseudo.png}
	\caption{Architecture of Pseudo-Distributed 
	Hadoop}\label{f:hadoop-pseudo}
\end{figure}

\paragraph{Direct Installation}
Direct installation of Hadoop is done based on instructions on
Dominique Thiebaut's Wiki~\cite{hid-sp18-419-Thiebaut} and from The
Apache Software
Foundation~\cite{hid-sp18-419-Apache-single-node}. First a virtual
machine running Ubuntu 16.04 is setup on VirtualBox running on the
2017 Macbook Pro described in Section~\ref{ss:pcs}. It was decided to
increase the memory to 2GB and the virtual hard drive to 10GB based on
instructions provided in the Handbook~\cite{las18handbook}. Networking
is configured using a bridged adaptor connected to the host's wifi
port. OpenSSH, Curl, and the Java JDK 8 from OpenJDK are installed on
the VM. Per Thiebault's instructions, ipv6 is disabled. A group
called \verb|hadoop| is created with a user called \verb|hduser|. SSH
keys are created using \verb|ssh-keygen| and hduser's public key is
added to \verb|known_hosts| so that passwordless SSH to to localhost
is possible. Pyenv is then installed and Python 3.6.2 is made the
global version of Python for the VM. Hadoop 2.9.0 is downloaded from
Apache and installed in \verb|/usr/local/hadoop|. The following
environment variables are set in hduser's \verb|.bashrc| file:
\begin{verbatim}
export HADOOP_HOME=/usr/local/hadoop
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
\end{verbatim}

The following directories are added to hduser's path:
\begin{verbatim}
$HADOOP_HOME/bin
$HADOOP_HOME/sbin
\end{verbatim}

The rest of the configuration is done with modifications to xml files
and scripts in \verb|$HADOOP_HOME/etc/hadoop|: the java location needs
to be set in \verb|hadoop_env.sh|, the detault file system is set
in \verb|core-site.xml|, and the number of file system replications is
set in \verb|yarn-site.xml|. To make it easier to clean everything up
and rerun the algorithm, the directory used for temporary files is
also set in \verb|core-site.xml|.

The sentiment analysis algorithm was run in pseudo-distributed mode on
the VM with and without yarn as a resource manager. If yarn is used, a
few more configuratons need to be done in the xml files
in \verb|$HADOOP_HOME/etc/hadoop|: the MapReduce framework name needs
to be set to yarn in \verb|mapred-site.xml|,
and \verb|mapreduce_shuffle| needs to be set up
in \verb|yarn-site.xml|.

\paragraph{Dockerized Installation} We used a Dockerfile to build the 
image for the Pseudo-Distributed Hadoop container. The image is
modified from the one on the sequenceiq GitHub
repository~\cite{hid-sp18-405-hadoop-sequenceiq}. The modification
includes minor bug fixes, change of the Java and Hadoop version used,
installation of Python and Pyenv, download of data and transfer of Python 
code for the use of Hadoop-Streaming. The container can be
started up with a shell interface for user to input commands operating
the cluster, or as an executable program which runs the whole
classification algorithm and transfers the results, including running
time, back to the host machine. In later case, the Docker Container
needs to be started by initiating a customized start-up script that
overrides the default scripts defined by the  \verb|COMMAND| or  
\verb|ENTRYPOINT|
settings in the Dockerfile.


\subsection{Fully-Distributed using Docker Compose}

A Fully-Distributed Hadoop cluster can be deployed on multiple VMs or
physical machines. By using Docker Compose, one can deploy the same
multi-node cluster on one single virtual machine.  The architecture is
illustrated in Figure~\ref{f:hadoop-compose}, where the red rectangle
stands for some virtual or physical machine. This could be a personal
laptop, VirtualBox running Ubuntu or FutureSystem Echo. However, since
Echo is in Swarm mode, Docker Compose will only deploy the hadoop
cluster on the head node due to the restriction that the only node
interacting with clients is the head node.

\begin{figure}[!ht]
	\centering\includegraphics[width=\columnwidth]{images/hadoop-docker-compose.png}
	\caption{Architecture of Fully-Distributed 
		Hadoop Cluster Using Docker Compose}\label{f:hadoop-compose}
\end{figure}

In Figure~\ref{f:hadoop-compose}, each Docker container acts as a
node and the cluster contains one Master Node and multiple Worker
Nodes. The ResourceManager and NameNode within the Master Node
container can communicate with the all the NodeManagers and DataNodes
respectively using TCP ports.

The configuration process differs from the Pseudo-Distributed Hadoop
container in two ways. First, there are two different start-up scripts
for Master Node container and Worker Node container respectively: the
script for Master Node only starts ResourceManager and NameNode but
not NodeManager or DataNode, and the script for Worker Node starts
NodeManager and DataNode but not ResourceManager or NameNode. Second,
a YAML file needs to be configured with information about the Master
and Worker Node containers that includes which image to use, which
ports to expose, the network, the container name, the start-up
command, etc. One important feature Docker Compose provides is that
one only needs to configure one Worker Node containers in the YAML
file instead of multiple ones. When starting up the service, a Docker
Compose scale command can be used to scale up the number of Worker
Nodes as desired. Furthermore, the scale command can also be used
while the cluster is running. In that case, the number of Worker Nodes
is adjusted according to the scale command therefore the size of
cluster can be controlled dynamically without the need of shutting
down and restart the cluster.

After the cluster is started up, a user can log into any of the
containers, check status, and run commands. The running of the
sentiment analysis algorithm is automated in a similar way as the
Pseudo-Distributed cluster using Docker Containers described in
Subsection~\ref{ss:pseudo-distributed}.


\subsection{Fully-Distributed using Docker Swarm}

Although Docker Compose provides a way to simplify the deployment of a
Fully-Distributed Hadoop cluster, there is an obvious limitation: all
the containers have to be on the same Docker Engine on an operating
system in order to be within the scope of the same VM or physical
machine. Therefore, the number of Worker Nodes and the computation and
storage capability of the cluster is limited by the host
machine. Docker Swarm, as introduced in Section~\ref{s:techused},
provides a way to deploy the cluster across different machines.

Instead of building a cluster of virtual machines with Docker Swarm
enabled, for this project we used the FutureSystem Echo, where Docker
Swarm has been fully implemented. The architecture of the Hadoop
cluster is illustrated in Figure~\ref{f:hadoop-swarm}, where each red
rectangle stands for a physical node other than the head node in the
swarm cluster because Docker Swarm will not deploy any container on
the head node. Although in this case, the Master Node is on a
different machine from some of the Worker Nodes, the communication is
enabled within the network which is automatically built as the first
step of the cluster deployment.  Similarly as Docker Compose, the
Swarm Mode also allows scaling up of the cluster without shutting down
the MapReduce process.

\begin{figure}[!ht]
	\centering\includegraphics[width=\columnwidth]{images/hadoop-docker-swarm.png}
	\caption{Architecture of Fully-Distributed 
		Hadoop Cluster Using Docker Swarm}\label{f:hadoop-swarm}
\end{figure}

The YAML file for Docker Compose can be used to configure the cluster
under the Swarm mode with some minor changes. Additional options, such
as deploy, are available, but some options, such as container name,
are disabled.  One key difference is that we have to include some
start-up scripts and use those as  \verb|Entrypoint| or  \verb|Command|to 
make the
sentiment analysis algorithm run automatically when the cluster
starts. Unlike in previous methods of deployment, where we still have
the option of logging into the container and pass commands through
bash, in swarm mode we cannot log into the containers because they are
all deployed on worker nodes of the Swarm cluster.

A challenge in implementing Swarm mode is determining how one gets the
analysis results, which sits on some worker node in the Swarm cluster,
back to the head node. A stardard way of doing this is to use the
WebHDFS REST API with the \verb|curl| command. However, in our
implementation, we had difficulty accessing different datanodes
where the results are actually stored through WebHDFS. This may be
caused by some port issue and configuration in the initial HDFS
setting. As a workaround we put the results together with the log
files in to the  \verb|logs| folder, which can be accessed through the YARN
ResourceManager on a certain port (8088 by default).


\section{Benchmarking Process}\label{s:benchrproc}

The benchmarking has been done on several machines listed in
Section~\ref{s:techused}: the 8GB RAM Macbook Pro 2015, the 16GB RAM
Acer laptop running Centos, the 16GB RAM desktop running Ubuntu on
VirtualBox, and FutureSystem Echo.  For each of these machines, we
used Docker to deploy Pseudo-Distributed Hadoop and Fully-distributed
Hadoop using Docker Compose for worker number starting at 1. For each
setting, 10 iterations of the whole process of setting up cluster and
running algorithm were performed. We then calculated the mean of
running time together with standard deviations and the coefficient of
variation (which is the ratio of standard deviation and mean).  On
Echo, we also benchmarked the Fully-Distributed Hadoop using Docker
Swarm in a similar fashion.

For some machines, there is a limit on the number of workers that can
be established. For example, the Macbook can have at most two
workers. When attempting to run more, the algorithm does not finish
because the machine runs out of memory. The two 16GB RAM Acer laptops
and VirtualBox on desktop could support 5 or 6 workers in a stable
fashion.  Stability here is defined as each iteration ending
successfully and the deviation in running time is relatively
small. Detailed results will be provided in Section~\ref{s:results}.

\section{Results}\label{s:results}
This section elaborates the result of our benchmarking
process. Deployment time and running time are discussed separately.

\subsection{Deployment Time}

All the Dockerized Hadoop clusters depend on Docker Engine and 
corresponding images. The deployment can be divided into three steps:

\begin{itemize}
	\item Install of Docker
	\item Build or pull the image or images
	\item Start the cluster
\end{itemize}

\paragraph{Install of Docker}
	The installation of Docker on Linux and MacOS operating
	systems following the official guide on Docker website and can
	be done in around 1 minute.
	
\paragraph{Build or pull the image or images}	
	Based on different Internet connection speeds we tried, one
	can build the images for deployment in 8 to 12 minutes. The
	images can also be pulled from DockerHub, which is the default
	cloud-based registry service of Docker. In our test, pulling
	in general is faster than building images by around 1
	minute. We have four images throughout the whole project:
	pseudo, base, master and worker.  The majority of layers are
	the same with some differences: Hadoop configuration
	files, startup scripts, and existence of data. Due to the sharing of
	layers, building 3 images (base, master and worker) for the
	Fully-Distributed cluster does not differ much from building
	only one. In the future, we could unify all images into one,
	with different scripts or options for the startup. It is
	noticed that on a single virtual machine if the iamges are
	pulled or built once, then later building process will utilize
	the cached layers and speed up the process.
	
\paragraph{Start the cluster}
	Starting a Pseudo-Distributed Hadoop takes less than 10
	seconds. The start-up time for the Fully-Distributed cluster
	increases with the number of workers initialized. For worker
	numbers smaller than 10, the start-up takes less than 20
	seconds. On FutureSystem Echo, under either Compose or Swarm
	mode, starting 40 workers takes around 30 seconds and starting
	100 workers takes 40 seconds.


From the breakdown of deployment steps, we can see that the build or pull
image step dominates the total time of the whole process. The total
time is not more than 13 minutes in total when building from
scratch. In our project, we also deployed a Pseudo-Distributed Hadoop
directly (without Docker) on a VirtualBox. The process of manually
setting up Hadoop with all dependencies and configuration files took
much longer than the dockerized version. This illustrated the
advantage of using Docker containers in cluster deployment.

\subsection{Running Time}

The running time is defined to be the time used to run the core script 
\verb|runPythonMapReduce.sh|, which includes four steps. The first step is 
to split the data randomly into training and testing sets with the
ratio 8:2 and while keeping the ratio of positive and negative reviews
unchanged (0.5). The second step is to create directories on HDFS and
put data into HDFS. The third step is the core MapReduce algorithm
described in Section~\ref{s:algorithm}. The last step is to fetch
classification results together with running time record from HDFS,
and transfer these from the container to the host machine.

The results are summarized in Table~\ref{t:results-table}, where each
row represents the number of workers established in the cluster and
each column is one particular deployment that is benchmarked. The
number in the table is the mean running time of the 10 iterations in
minutes (m) and seconds (s).  The standard deviation is listed in the
brackets. For each deployment options, the bold number is the least
average running time that we observed, which can be considered as the
optimal Docker cluster choice for that deployment. The asterisk (*) indicates 
that the standard deviation is large. The
threshold we used is that coefficient of variance (standard deviation
divided by mean) equals to 0.1.

\begin{table}[hbt]
\centering
\caption{Running Time results}\label{t:results-table}
	\begin{tabular}{llllll}
		\toprule
		\# Workers & Macbook Pro 2015 & Acer Laptop & Virtualbox on 
		Desktop & Echo (Compose) & Echo (Swarm) \\ \midrule
		Pseudo & \textbf{99m14s (80s)} & \textbf{77m15s (8s)} & 43m34s 
		(39s) & 22m25s (12s) & N/A \\\midrule
		1 & 104m34s (119s) & 82m8s (57s) & 44m47s (20s) & 33m42s (12s) & 
		20m49s (20s) \\\midrule
		2 & 108m50s (110s) & 84m35s (7s) & \textbf{42m47s (24s)} & 18m57s 
		(6s) & 15m26s (17s) \\\midrule
		3 & N/A & 86m5s (6s) & 43m13s (17s) & 17m41s (2s) & 11m14s (4s) 
		\\\midrule
		4 & N/A & 87m50s (11s) & 43m59s (58s) & \textbf{17m38s (2s)} & 
		9m24s (3s) \\\midrule
		5 & N/A & 90m1s (8s) & 48m46s (104s) & 17m52s (2s) & 8m47s (6s) 
		\\\midrule
		6 & N/A & 92m32s (11s) & 58m57s (171s) & 18m9s (2s) & 8m25s (7s) 
		\\\midrule
		7 & N/A & N/A & 85m19s (634s)* & 18m17s (2s) & 8m2s (4s) \\\midrule
		8 & N/A & N/A & 113m36s (2520s)* & 18m28s (2s) & 7m44s (2s) 
		\\\midrule
		9 & N/A & N/A & N/A & 18m29s (1s) & 7m39s (2s) \\\midrule
		10 & N/A & N/A & N/A & 18m34s (2s) & \textbf{7m35s (2s)} \\\midrule
		15 & N/A & N/A & N/A & 18m56s (1s) & 7m37s (3s) \\\midrule
		20 & N/A & N/A & N/A & 19m12s (1s) & 7m44s (3s) \\\midrule
		25 & N/A & N/A & N/A & 19m34s (2s) & 7m52s (2s) \\\midrule
		30 & N/A & N/A & N/A & 19m50s (2s) & 8m1s (4s) \\\midrule
		35 & N/A & N/A & N/A & 20m2s (3s) & 8m16s (4s) \\\midrule
		40 & N/A & N/A & N/A & 20m18s (3s) & 8m20s (4s)\\\bottomrule
	\end{tabular}
\end{table}

\section{Observation and Discussion}
Based on Table~\ref{t:results-table}, we have several observations: 

\subsection{Bottleneck of Memory} 

The first three deployments are done on personal computers, with
memory from 8G to 16G. The N/A in the table indicates that the
algorithm fails at run time due to Java exceptions (out of memory or
lost of connection to MapReduce containers), which are good evidence
of memory shortage. We observe that the Macbook, which has the least
memory, can only run two Worker Node containers. The Acer laptop and
desktop both have 16G memory, and they can sustain up to 6 Worker Node
containers. Although we also get results for the desktop with 7 and 8
workers established, the running time increases significantly, and the
variances are very large. Indeed, we could not get results on all 10
iterations with 8 workers, only 4 out of 10 executed
successfully. During the execution with 7 or 8 workers, we can see
from the terminal that some MapReduce containers are killed due to
memory issue or connection issue and in that case ResourceManager will
assign the failed jobs again until they are finished. During each
iteration, different number of failed containers causes the big
variation in the total running time.

Echo head node has almost 400GB of memory, so the iterations on 
Echo in Compose mode runs up to 40 Workers without any memory issue. 
When we deploy the cluster in Swarm mode, the total memory of the system 
is even larger (4 physical servers combined). However, since there may be 
other projects running on Echo, we did not increase the number of the 
Worker Node to test the limits of the system. 

\subsection{Pseudo vs One-Worker} 

We did not deploy Pseudo-Distributed Hadoop on Swarm because it would 
be only on one physical machine and does not utilize the advantages of 
Swarm. For all other deployment, we noticed that Pseudo-Distributed 
performs faster than the Fully-Distributed cluster with only one worker.

In these two settings, there will be exactly one instance of DataNode,
NameNode, ResourceManager and NodeManager, the only difference is that
in Fully-Distributed cluster with only one worker, these JVM runs in
different containers. Given that both deployments share the same
memory, computing power, and disks, the differences in the performance
are likely related to the overhead of communication across
containers. For example, message between DataNode and NameNode in the
one-worker setting will have to pass through some TCP ports across
containers while in Pseudo mode, the communication is much easier
between JVMs directly. Therefore we see that for Pseudo-Distributed
Hadoop outperforms Fully-Distributed Hadoop with only one worker.

\subsection{Optimal Number of Worker}
In each deployment with Fully-Distributed clusters, it seems that as
the number of workers increases from one, the running time first
decreases and then starts to increase. Therefore, each deployment has
an optimal number of workers. The decrease and increase pattern in
running time can be explained as follows: if the number of MapReduce
containers (JVMs) within one Worker Node is limited by the settings
in \verb|mapred-site.xml|, then increase of Workers could lead to more
mapper and/or reducers working at the same time thus reducing running
time. However, since the number of mapper and reducer containers
needed for the task is fixed (for example, 800 training files need at
most 800 mapper), increasing number of workers does not really help
because all of the workers are sharing the same set of processors and
computing power. On the other hand, all workers also share the same
hard drives, and the intensive file I/O could be the reason for
the decline in performance as number of workers grows.
(As discussed in Section~\ref{s:algorithm}, all mappers and reducers read
files and write files to the disk extensively.) This is our conjecture
based on the results observed and understanding of the algorithm; to
make a conclusion of the root cause of this pattern, more specific
testing with better control groups needs to be done.

Macbook and Acer laptop have the optimal number of workers to be one.
According to previous discussion, one-worker clusters are slower than
Pseudo-Distributed clusters. Therefore, running a Fully-Distributed
cluster on these machines to solve the required sentiment analysis
task is inefficient.  The Desktop achieves the best result when the
number of worker is 2. The Echo head node (when using Docker Compose)
is optimal at 4 workers, and in Swarm mode on Echo the optimal worker
number occurs within the range of 10 to 15.

\subsection{Same Cluster on Different Platforms}

If we fix the number of workers in the Fully-Distributed cluster, and
compare the performance across these platforms. From Acer laptop,
desktop, Echo with Compose and Echo with Swarm, the running time
roughly reduces by half. We conjecture that the reason for the
differences could have two possibilities, CPU power and Disk
read/write speed. Given same number of Workers (without memory issue),
the workload of each mapper and reducer is similar across platforms,
communication overhead should also be comparable, the differences
could then be CPU power or hard drive writing speed or both. To
disentangle these effect and draw a conclusion of the actual cause, we
may need to design experiments controlling one of the effect and test
the other. This could be future steps of the project.

Regardless of number of workers chosen, deployment under Swarm mode
really outperforms the other. This illustrates the power of cloud
computing in general. We could imagine gigantic clusters in Swarm mode
hosting large number of containers, provides timely solution for Big
Data applications.


\section{Conclusion}\label{s:conclusion}

Although its popularity is waning, Hadoop remains a useful tool for
Big Data analysis. As one would expect, better hardware leads to
better results.  While many of the configurations tested are not ideal
for the task, using a combination of Docker in its Compose and Swarm
variants and Hadoop, provides a robust and versitile platform that can
be deployed on a wide variety of hardware and software configurations
with very little custom configuration. Startup is fast and the
results, when there is sufficient memory available, are consisent. The
authors would like to continue our exploration of Hadoop, comparing
other cloud environments, including the Raspberry Pi cluster
configuration that we began implementing and with larger datasets. 

\section{Appendix: Hadoop Cluster on Raspberry Pi}\label{s:appendix}
This section describes steps taken toward the goal of creating a
Hadoop cluster using 5 Raspberry Pi 3 Model B computers in a
repeatable, scalable way, so that the process could be replicated to
create a much larger cluster. Successes and challenges are described
and recommended next steps are outlined.

At the end of the process, each Pi has a unique hostname a new
password. A computer outside of the cluster can log into any of the
Pis via ssh and the Pis are configured to run MapReduce jobs using
Apache Hadoop. The process of achieving these goals manually has been
well documented in various blogs and message boards on the
internet~\cite{hid-sp18-419-headless}, but in order to make the
process scalable, the confuguration described is automated to a large
extent.

\subsection{Hardware Used}

The hardware used for the Raspberry Pi cluster consists of:

\begin{itemize}
\item Five Raspberry Pi 3 Model B computers 
\item One Waveshare 4in HDMI LCD touchscreen
\item One Netgear model GS308 8-Port Gigabit ethernet switch
\item One Anker PowerPort 6 USB power supply
\item One Powtech 125V AC 15A 1875w adaptor with switch
\item Five 1-foot ethernet cables
\item Five 32 GB microSDHC UHS-I cards
\item Six 6in USB 2.0 A-Male to Micro B cables
\item 24 20mm by 5mm Hex Hexagonal Threaded Spacer Supports
\end{itemize}

A pinout diagram of the Pi 3B is shown in Figure~\ref{f:pinout-diagram}.

\begin{figure*}[!ht]
  \centering\includegraphics[width=\columnwidth]{images/raspberry_pi_circuit_note_fig2.png} \caption{Pinout
  diagram of Raspberry Pi 3B~\cite{hid-sp18-419-pi-pinout}. Note: the
  Pi used for this project has a Broadcom BCM2837, not the Broadcom
  BMC2835 shown in this figure.}\label{f:pinout-diagram}
\end{figure*}

\subsection{Building a Pi Cluster}
First, aluminium and copper heat syncs need to be attached to each
Pi. The two aluminium heat syncs are attached to the Broadcom chip and
the SMSC ethernet controller located on the top of the Pi. The blades
of the heat syncs are parallel to the longer side of the Pi as shown
in Figure~\ref{f:heat-sync-top}.

\begin{figure*}[!ht]
  \centering\includegraphics[width=\columnwidth]{images/heat-sync-top.jpg} \caption{Top
  view of a Raspberry Pi 3B with heat syncs
  attached.}\label{f:heat-sync-top}
\end{figure*}

The flat copper heat sync is attached to the Elpida RAM on the bottom
of the Pi as shown in Figure~\ref{f:heat-sync-bottom}.

\begin{figure*}[!ht]
  \centering\includegraphics[width=\columnwidth]{images/heat-sync-bottom.jpg} \caption{Bottom
  view of a Raspberry Pi 3B with heat sync
  attached.}\label{f:heat-sync-bottom}
\end{figure*}

After attaching the heat syncs, threaded hexagonal spacer supports are
used to connect the Pis together. A fully-assembled 5-node Pi cluster
is shown in Figure~\ref{f:cluster-no-wires}.

\begin{figure*}[!ht]
  \centering\includegraphics[width=\columnwidth]{images/pi-cluster-no-wires.jpg}
  \caption{5-node Pi cluster before wiring.}\label{f:cluster-no-wires}
\end{figure*}

Each node of the cluster is then attached to the switch using an
ethernet cables and to the power supply using a usb cables. The fully
wired cluster is shown in Figure~\ref{f:complete-cluster}.

\begin{figure*}[!ht]
  \centering\includegraphics[width=\columnwidth]{images/complete-pi-cluster.jpg}
  \caption{Fully assembled 5-node Pi cluster.}\label{f:complete-cluster}
\end{figure*}

\subsection{Creating a Custom Image}
Each Pi uses a 32GB SD card as its hard drive. The operating system is
to be burned on each SD card. For this cluster, the most current
version of Raspbian Stretch Lite is chosen as the operating
system. When downloaded, this image has ssh disabled and is configured
with one user \verb|pi| with the password \verb|raspberry|. Every
machine is named \verb|raspberrypi|. In order to easily determine
which SD card is in which computer in the cluster, the names are
changed before burning the image to the SD card, and ssh is enabled on
each card so the rest of the configuraton can be done from a remote
computer without using a keyboard or monitor, commonly known as
headless setup.

Another approach that the authors would like to explore at a later
date would be to compile a custom image from the Raspbian source
code. The code is freely available to download, either directly from
raspbian.org~\cite{hid-sp18-419-raspbian-distro} or on
GitHub~\cite{hid-sp18-419-pi-gen}.

The computer used to create the custom images is the 2017 Macbook Pro
that is described in Subsection~\ref{ss:pcs}. The script that creates
the custom images downloads the latest image from Raspbian.org and
mounts the two partitions so that they can be modified. The process
used to mount the second partion only works on Linux, so a VM is
created on which the script is run. This VM needs to have a big enough
virtual disk for the base image times the number of custom images
created. The current unzipped Raspbian Stretch Lite image at the time
of this writing is 1.9GB. Instructions for setting up the VM provided
in the Handbook were used~\cite{las18handbook}.

If the images are going to be burned and verified on the VM,
additional space will be needed equal to the size of the SD cards
being burned because verificaton is done by copying the entire
capacity of the SD card and comparing it to the image that was
burned. The SD cards we are using are 32GB, so creating 5 images and
verifying the burn will require around 44GB on top of the space
required for Ubuntu. To be safe, the VM for creating the custom images
is configured with at least a 65GB virtual disk image hard drive.  It
runs Ubuntu 16.04 on VirtualBox and is configured with 1 CPU, 2GB of
memory, and USB 3.0 enabled. Networking is via a bridged network
connected to the hosts WiFi.

The images are created using two scripts: \verb|download_image.sh| - a
simple shell script to download the latest Raspbian Stretch Lite
image, and \verb|modify_sdcard.py| - a Python script that will create
the custom images. The Python script runs on Python version 2.7.12,
which is the version that comes installed on Ubuntu 16.04. It requires
one additional package, \verb|pycryptodome|, which is needed to
generate ssh keys. The script needs to be run as root to have
permission to mount and modify the files in the Raspbian image.

The Python script creates the number of custom images specified with a
flag. The names of the images are a basename followed by a three-digit
number that increments sequentially from the specified starting number
through that number plus the number of images created. The basename,
starting number, and number of images can be specified with flags. The
basename defaults to \verb|snowcluster| and the starting number
defaults to 0, so if three images were specified, the result would be
three files: \verb|snowcluster000.img|, \verb|snowcluster001.img|,
and \verb|snowcluster002.img|.

When the script is run, the base image is copied the number of times
specified by the user and the following modifications are made: * If
the \verb|--ssh| flag is set, or by default, the script will mount the
first partition of the image and add a blank file
named \verb|ssh|. This enables ssh on the Pi on first boot. It also
generates public and private RSA keys, creates a
folder \verb|//home/pi/.ssh/| and stores those keys in files
called \verb|id_rsa| for the private key and \verb|id_rsa.pub| for the
public key. It then appends the public key to the
file \verb|/home/pi/.ssh/authorized_keys|. If that file does not yet
exist, it is created. This allows passwordless ssh login between any
of the Pis in the cluster.  * If the user specifies a public key with
the \verb|--sshkey| flag, the contents of that file is also appended
to the file \verb|/home/pi/.ssh/authorized_keys|. This allows
passwordless ssh into the any of the Pis in the cluster from a setup
machine.

After the images are created, they can either be copied to a shared
folder and burned to the SD cards on the Mac using Etcher, or on the
VM using \verb|dd|. Instructions for doing this provided by The
Raspberry Pi Foundation were tested on both
platforms~\cite{hid-sp18-419-raspbian-burn}.

A known issue with the Python script is that because it needs to be
run as root to mount and unmount the images and modify the
contents, \verb|~/.ssh| and its contents: \verb|id_rsa|,
\verb|id_rsa.pub|, and \verb|authorized_keys| are owned by root.
This leads to a permissions error when connecting via ssh from one pi
to another. Solutions for this issue need to be investigated.

In addition to fixing this permissions issue, other desired future
enhancements include:

\begin{itemize}
        \item Adding an option to specify paramaters in a yaml file as
        specifying them on the command line is cumbesome.
        \item Adding an option to create keys and adding them
        to \verb|known_hosts| on all the images to bypass the
        verification on first ssh connection.
        \item Setting a fixed
        IP address on the head node if DHCP is to be used, or for all
        the Pis.
        \item Incorporating the functionality
        in \verb|download_image.sh| into script, with option to
        download a different base OS (e\.g\. from Dexter Labs).
        \item Adding an option to burn the images from the script.
        \item Adding a script to do post-boot configuraton.
\end{itemize}

\subsection{Post-boot configuraton}
Post-boot configuration was explored but not implemented. The goal for
future development would be to write a script that would perform the
remainder of the configuration, copy it to each of the Pis, and run on
first login. This script will permanently enable ssh; change the
password of all of the Pis; create or update a database containing IP
addresses, Mac addresses, and hostnames; optionally configure the head
node of the cluster as a DHCP server; and install and configure Hadoop
on the Pis.

Permanently enabling ssh is necessary as the \verb|ssh| file added in
to the first partition is deleted after first login. It is done with
the following commands:
\begin{verbatim} 
sudo apt-get update
sudo apt-get install openssh-server
sudo systemctl enable ssh
sudo systemctl start ssh
\end{verbatim}

Changing the password from the default password \verb|raspberry| to
something more secure can be done with this
line~\cite{hid-sp18-419-so-password}:
\begin{verbatim} 
echo -e ``raspberry\\nsnowcluster\\nsnowcluster'' | passwd
\end{verbatim}

The database of Mac addresses, IP addresses, and hostnames can be done
in two steps: populate the hostnames and IP addresses during the
creation of the VM images, and add the Mac addresses after booting up
the Pis. Then the setup script can run \verb|ifconfig -a| and parse
the output to get the Mac addresses for both wireless and wired
connections. 

Setting up head node of the cluster as a DHCP server requires further
investigation and testing. The process was followed and documented,
but a new version of the DHCP server \verb|isc-dhcp-server| is
incompatible with the process followed~\cite{hid-sp18-419-pi-DHCP}.

\section{Work Breakdown}
Min Chen converted the sentiment analysis algorithm, did the Docker
configurations and benchmarking and was the main author of these
sections of the paper. Bertolt Sobolik did the Pi configuration and
pseudo-distributed configurations on the virtual machines and was the
main author of these sections of the paper. Min Chen and Bertolt
Sobolik review and tested each other's work.

\begin{acks}

  The authors would like to thank Dr.~Gregor~von~Laszewski for his
  support and suggestions to write this paper, and Bo Feng for his guidance 
  and help with Hadoop deployment in Docker Swarm mode.

\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{report}
